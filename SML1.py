# -*- coding: utf-8 -*-
"""project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P5BKYAzqNfGr9ZcSGymRR_5PrqZ-uxlX
"""

# Display images from the digits data set

import matplotlib.pyplot as plt
from sklearn import datasets

# import some data to play with
digits = datasets.load_digits()
print("Class: " + str(digits['target'][5]))
print(digits.data.shape[0])
plt.gray()
plt.matshow(digits.images[5])
plt.show()

print(digits['target'][:])

"""
=====================================================
Stochastic subgradients for training SVM
=====================================================
"""

import numpy as np
import matplotlib.pyplot as plt
import math
from sklearn import datasets
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process.kernels import DotProduct



def svmsubgradient(Theta, x, y):
#  Returns a subgradient of the objective empirical hinge loss
#
# The inputs are Theta, of size n-by-K, where K is the number of classes,
# x of size n, and y an integer in {0, 1, ..., 9}.
    G = np.zeros(Theta.shape) # ()10,64)
    delta = []
    class_score = np.matmul(x, Theta)
    for k in range(10):
      if k==int(y):
        delta.append(-1)
        continue
      else:
        delta.append(np.maximum(0,class_score[k] - class_score[int(y)] + 1))
        
    j = delta.index(max(delta))
    if delta[j] > 0:
      G[:,y] = -x
      G[:,j] = x
    return(G)
 
 
  
def sgd(Xtrain, ytrain, maxiter = 10, init_stepsize = 1.0, l2_radius = 10000):
#
# Performs maxiter iterations of projected stochastic gradient descent
# on the data contained in the matrix Xtrain, of size n-by-d, where n
# is the sample size and d is the dimension, and the label vector
# ytrain of integers in {0, 1, ..., 9}. Returns two d-by-10
# classification matrices Theta and mean_Theta, where the first is the final
# point of SGD and the second is the mean of all the iterates of SGD.
#
# Each iteration consists of choosing a random index from n and the
# associated data point in X, taking a subgradient step for the
# multiclass SVM objective, and projecting onto the Euclidean ball
# The stepsize is init_stepsize / sqrt(iteration).
    
    K = 10
    NN, dd = Xtrain.shape
  
    Theta = np.zeros(dd*K)
    Theta.shape = dd,K
    mean_Theta = np.zeros(dd*K)
    mean_Theta.shape = dd,K
    np.random.seed(1)
    #np.random.seed(173)
    
    for k in range(maxiter):
      i = np.random.randint(0,NN)
      if k == 0:
        eps = init_stepsize
      else:
        eps = init_stepsize/math.sqrt(k)
      G = svmsubgradient(Theta, Xtrain[i,:], y[i])
      Theta -= eps*G
      mean_Theta = (mean_Theta*k + Theta)/(k+1)
      l2_norm = math.pow(np.linalg.norm(Theta,2),2)
      if l2_norm > math.pow(l2_radius,2):
        Theta = Theta*(math.pow(l2_radius,2)/l2_norm)
    return Theta, mean_Theta

def Classify(Xdata, Theta):
#
# Takes in an N-by-d data matrix Adata, where d is the dimension and N
# is the sample size, and a classifier X, which is of size d-by-K,
# where K is the number of classes.
#
# Returns a vector of length N consisting of the predicted digits in
# the classes.
    scores = np.matmul(Xdata, Theta)
    inds = np.argmax(scores, axis = 1)
    return(inds)


maxiter = 40000
#init_stepsize = 1

# import some data to play with
digits = datasets.load_digits()

# Load data into train set and test set
digits = datasets.load_digits()
X = digits.data
y = np.array(digits.target, dtype = int)
N,d = X.shape
Ntrains = [20, 50, 100, 500, 1000, 1500, 1697]
Ntest = 100
l2_radius = 40.0
error_rate = []
for Ntrain in Ntrains:
  
  Xtrain = X[0:Ntrain-1,:]
  ytrain = y[0:Ntrain-1]
  Xtest = X[(N-100):N,:]
  ytest = y[(N-100):N]

  M_raw1 = np.linalg.norm(Xtrain)
  M_raw = np.sqrt(np.sum(np.square(Xtrain)))
  init_stepsize = l2_radius/M_raw

  Theta, mean_Theta = sgd(Xtrain, ytrain, maxiter, init_stepsize, l2_radius)
  print('Error rate for ' + str(Ntrain) + ' training samples')
  error_rate.append(np.sum(np.not_equal(Classify(Xtest, mean_Theta),ytest)/Ntest))
  print(error_rate[-1])
  
fig = plt.figure()
fig.suptitle('SVM Test error rate', fontsize=20)
plt.xlabel('Number of training samples', fontsize=16)
plt.ylabel('Test error rate', fontsize=16)
plt.plot(Ntrains, error_rate)
#fig.savefig('test.jpg')
plt.show()

print(init_stepsize)
print(M_raw1)

maxiter = 40000
#init_stepsize = 1

# import some data to play with
digits = datasets.load_digits()

# Load data into train set and test set
digits = datasets.load_digits()
X = digits.data
y = np.array(digits.target, dtype = int)
N,d = X.shape
Ntrains = [20, 50, 100, 500, 1000, 1500]
Ntest = 100
l2_radius = 40.0
error_rate = []
Ntrain = 1500
initial_step_sizes = [0.8]
#initial_step_sizes = [1, 0.8, 0.5, 0.25, 0.1, 0.05, 0.025, 0.01, 0.005]
Xtrain = X[0:Ntrain-1,:]
ytrain = y[0:Ntrain-1]
Xtest = X[(N-100):N,:]
ytest = y[(N-100):N]

for x in initial_step_sizes:
  init_stepsize = x

  Theta, mean_Theta = sgd(Xtrain, ytrain, maxiter, init_stepsize, l2_radius)
  print('Error rate with initial step size ' + str(x))
  error_rate.append(np.sum(np.not_equal(Classify(Xtest, mean_Theta),ytest)/Ntest))
  print(error_rate[-1])
  
fig = plt.figure()
fig.suptitle('SVM Test error rate', fontsize=20)
plt.xlabel('Initial step size', fontsize=16)
plt.ylabel('Test error rate', fontsize=16)
#plt.plot(Ntrains, error_rate)
plt.plot(initial_step_sizes,error_rate)
#fig.savefig('test.jpg')
plt.show()

"""
=====================================================
Confusion matrix for SVM
=====================================================
"""

import sklearn.metrics as metrics
import pandas as pd
import seaborn as sn
#Confusion matrix to better evaluate the classification
yp_test = Classify(Xtest, mean_Theta)
con_mat = metrics.confusion_matrix(ytest, yp_test)
norm = []
for i in range(len(con_mat)):
  norm.append([float(x)/sum(con_mat[i]) for x in con_mat[i]])
df_cm = pd.DataFrame(norm, range(10), range(10))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 16})# font size

"""
=====================================================
Gaussian process classification (GPC) 
=====================================================

"""

import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn import datasets
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process.kernels import DotProduct
# import some data to play with
digits = datasets.load_digits()

X = digits.data
y = np.array(digits.target, dtype = int)

N,d = X.shape # N = 1797

#Ntrain_vec = [1500]
Ntrain_vec = [20,50,100,500,1000,1500]

for i in Ntrain_vec:

  #N = np.int(1000)
  Ntrain = np.int(i)
  #Ntest = np.int(100)


  Xtrain = X[0:Ntrain-1,:]
  ytrain = y[0:Ntrain-1]
  Xtest = X[Ntrain:N,:]
  ytest = y[Ntrain:N]

  #kernel = 1.0 * RBF([20.0]) # isotropic kernel #Test error rate = 0.89
  kernel = DotProduct(20.0) #Test error rate = 0.14
  gpc_rbf = GaussianProcessClassifier(kernel=kernel).fit(Xtrain, ytrain)
  yp_train = gpc_rbf.predict(Xtrain)

  train_error_rate = np.mean(np.not_equal(yp_train,ytrain))
  yp_test = gpc_rbf.predict(Xtest)
  test_error_rate = []
  test_error_rate.append(np.mean(np.not_equal(yp_test,ytest)))

  print('Training error rate')
  print(train_error_rate)
  print('Test error rate')
  print(test_error_rate)

"""
=====================================================
Confusion matrix for GP regression
=====================================================
"""

import sklearn.metrics as metrics
import pandas as pd
import seaborn as sn
#Confusion matrix to better evaluate the classification




con_mat = metrics.confusion_matrix(ytest, yp_test)
norm = []
for i in range(len(con_mat)):
  norm.append([float(x)/sum(con_mat[i]) for x in con_mat[i]])
df_cm = pd.DataFrame(norm, range(10), range(10))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 16})# font size

"""
=====================================================
2b. Gaussian process regression 
=====================================================

"""


import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn import datasets
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process.kernels import DotProduct
from sklearn.gaussian_process import GaussianProcessRegressor
# import some data to play with


def indices_to_one_hot(data, nb_classes):
    return (np.eye(nb_classes)[np.array(data)])

digits = datasets.load_digits()


X = digits.data
y = np.array(digits.target, dtype = int)

N,d = X.shape # N = 1797

#Ntrain_vec = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700]
Ntrain_vec = [20,50,100,500,1000,1500]
#Ntrain_vec = [1500]
test_error_rate_vec = np.zeros(len(Ntrain_vec))

for i in range(len(Ntrain_vec)):
  N = np.int(1797)
  Ntrain = np.int(Ntrain_vec[i])


  Xtrain = X[0:Ntrain-1,:]
  ytrain = y[0:Ntrain-1]
  Xtest = X[Ntrain:N,:]
  ytest = y[Ntrain:N]



  Y_train = indices_to_one_hot(ytrain, 10)
  Y_test = indices_to_one_hot(ytest, 10)


  kernel = 1.0 * RBF([5]) # isotropic kernel #Best Test error rate = 0.02 when Size of training data = 900 
  #kernel = DotProduct(1.0) # Best Test error rate = 0.167 when size of training data = 700 | Linalg crasch when we have size of training = 1000
 
  
  
  gpc_rbf = GaussianProcessRegressor(kernel=kernel).fit(Xtrain, Y_train)
  yp_train = gpc_rbf.predict(Xtrain)

  YP_train = indices_to_one_hot(yp_train.argmax(axis=1), 10)


  train_error_rate = np.mean(np.not_equal(YP_train,Y_train))
  yp_test = gpc_rbf.predict(Xtest)

  YP_test = indices_to_one_hot(yp_test.argmax(axis=1), 10)
  test_error_rate = np.mean(np.not_equal(np.argmax(YP_test, axis=1), np.argmax(Y_test, axis=1)))

  YP_class_test = yp_test.argmax(axis=1)

  print('Training error rate')
  print(train_error_rate)
  print('Test error rate,', "Size of training data:", Ntrain_vec[i])
  print(test_error_rate)
  test_error_rate_vec[i] = test_error_rate

  
plt.plot(Ntrain_vec, test_error_rate_vec)
plt.show()

Ntrain_vec = [100, 200, 300, 400, 500, 600, 700, 800, 900]
print(type(Ntrain_vec[0]))

"""
=====================================================
Confusion matrix for GP regression
=====================================================
"""

import sklearn.metrics as metrics
import pandas as pd
import seaborn as sn
#Confusion matrix to better evaluate the classification

con_mat = metrics.confusion_matrix(ytest, YP_class_test)
norm = []
for i in range(len(con_mat)):
  norm.append([float(x)/sum(con_mat[i]) for x in con_mat[i]])
df_cm = pd.DataFrame(norm, range(10), range(10))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 16})# font size



"""
=====================================================
Convolutional Neural Network Classification (CNNC) - Part a)
=====================================================
"""
from sklearn import datasets
import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import sklearn
import numpy as np
import sklearn.metrics as metrics

digits = datasets.load_digits()

X = digits.data
X /= 16
y = np.array(digits.target, dtype = int)

N,d = X.shape # N = 1797

Ntrain = 1697
Ntest = 100
  
Xtrain = X[0:Ntrain-1,:]
ytrain = y[0:Ntrain-1]
Xtest = X[Ntrain:N,:]
ytest = y[Ntrain:N]
Xtrain = Xtrain.reshape(Xtrain.shape[0], 8, 8 , 1).astype('float32')
Xtest = Xtest.reshape(Xtest.shape[0], 8, 8 , 1).astype('float32')

np.random.seed(1)
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(10, kernel_size=3, activation=tf.nn.relu),
  tf.keras.layers.Conv2D(10, kernel_size=3, activation=tf.nn.relu),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(Xtrain, ytrain, epochs=10)
print(model.evaluate(Xtest, ytest))

#Confusion matrix to better evaluate the classification
Ypred = model.predict_classes(Xtest, verbose=1)

con_mat = metrics.confusion_matrix(ytest, Ypred)
norm = []
for i in range(len(con_mat)):
  norm.append([float(x)/sum(con_mat[i]) for x in con_mat[i]])
df_cm = pd.DataFrame(norm, range(10), range(10))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 16})# font size



model.summary()

import sklearn.metrics as metrics
#Confusion matrix to better evaluate the classification
Ypred = model.predict_classes(Xtest, verbose=1)

con_mat = metrics.confusion_matrix(ytest, Ypred)
norm = []
for i in range(len(con_mat)):
  norm.append([float(x)/sum(con_mat[i]) for x in con_mat[i]])
df_cm = pd.DataFrame(norm, range(10), range(10))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 16})# font size

"""
=====================================================
Convolutional Neural Network Classification (CNNC) - Part b)
=====================================================
"""
from sklearn import datasets
import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import sklearn
import numpy as np

digits = datasets.load_digits()

X = digits.data
X /= 16
y = np.array(digits.target, dtype = int)

N,d = X.shape # N = 1797

Ntrains = [20, 50, 100, 500, 1000, 1500]
Ntest = 100
error_rate = []
for Ntrain in Ntrains:
  
  Xtrain = X[0:Ntrain-1,:]
  ytrain = y[0:Ntrain-1]
  Xtest = X[(N-100):N,:]
  ytest = y[(N-100):N]
  Xtrain = Xtrain.reshape(Xtrain.shape[0], 8, 8 , 1).astype('float32')
  Xtest = Xtest.reshape(Xtest.shape[0], 8, 8 , 1).astype('float32')

  model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(128, kernel_size=3, activation=tf.nn.relu),
    tf.keras.layers.Conv2D(128, kernel_size=3, activation=tf.nn.relu),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
  ])

  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

  model.fit(Xtrain, ytrain, epochs=100)
  error_rate.append(1 - model.evaluate(Xtest, ytest)[1])
  print('Error rate for ' + str(Ntrain) + ' training samples: ' + str(error_rate[-1]))

print(error_rate)
fig = plt.figure()
fig.suptitle('CNN Test error rate', fontsize=20)
plt.xlabel('Number of training samples', fontsize=16)
plt.ylabel('Test error rate', fontsize=16)
plt.plot(Ntrains, error_rate)
plt.show()

from sklearn import datasets
import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import sklearn
import numpy as np

digits = datasets.load_digits()

X = digits.data
X /= 16
y = np.array(digits.target, dtype = int)

N,d = X.shape # N = 1797

Ntrain = 1500
Ntest = 100
error_rate = []

Xtrain = X[0:Ntrain-1,:]
ytrain = y[0:Ntrain-1]
Xtest = X[(N-100):N,:]
ytest = y[(N-100):N]
Xtrain = Xtrain.reshape(Xtrain.shape[0], 8, 8 , 1).astype('float32')
Xtest = Xtest.reshape(Xtest.shape[0], 8, 8 , 1).astype('float32')

"""
Error rate for 1697 training samples: 0.020000000000000018
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(50, kernel_size=3, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.1),
  tf.keras.layers.Conv2D(50, kernel_size=3, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.05),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
"""


#Error rate for 1500 training samples: 0.030000000000000027
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(200, kernel_size=3, activation=tf.nn.relu),
  tf.keras.layers.Conv2D(80, kernel_size=3, activation=tf.nn.relu),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
"""
Error rate for 1500 training samples: 0.050000000000000044
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(200, kernel_size=3, activation=tf.nn.relu),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
"""
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(Xtrain, ytrain, epochs=100)
error_rate.append(1 - model.evaluate(Xtest, ytest)[1])
print('Error rate for ' + str(Ntrain) + ' training samples: ' + str(error_rate[-1]))

import sklearn.metrics as metrics
#Confusion matrix to better evaluate the classification
Ypred = model.predict_classes(Xtest, verbose=1)

con_mat = metrics.confusion_matrix(ytest, Ypred)
norm = []
for i in range(len(con_mat)):
  norm.append([float(x)/sum(con_mat[i]) for x in con_mat[i]])
df_cm = pd.DataFrame(norm, range(10), range(10))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 16})# font size
model.summary()

# Added 28/11 11.33 just to double check the GP classifier results

import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn import datasets
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process.kernels import DotProduct
# import some data to play with
digits = datasets.load_digits()

X = digits.data
y = np.array(digits.target, dtype = int)

N,d = X.shape # N = 1797

N = np.int(1797)
Ntest = np.int()
Ntrain_vec = np.array([20, 50, 100, 500, 1000, 1500])
#kernel_vec = np.array([1, 3, 5, 10, 15, 20])
kernel_vec = np.array([1])
Xtest = X[N-100:N,:]
ytest = y[N-100:N]
test_error_rate_mat = np.zeros((len(Ntrain_vec),len(kernel_vec)))

for i in range(len(Ntrain_vec)):
    Ntrain = np.int(Ntrain_vec[i])
    Xtrain = X[0:Ntrain-1,:]
    ytrain = y[0:Ntrain-1]
    for j in range(len(kernel_vec)):
        kernel = 1.0 * RBF([kernel_vec[j]]) # isotropic kernel #Test error rate = 0.89
        #kernel = DotProduct(kernel_vec[j]) #Test error rate = 0.14
        gpc_rbf = GaussianProcessClassifier(kernel=kernel).fit(Xtrain, ytrain)
        yp_train = gpc_rbf.predict(Xtrain)
        yp_test = gpc_rbf.predict(Xtest)
        test_error_rate = np.mean(np.not_equal(yp_test,ytest))
        test_error_rate_mat[i,j] = test_error_rate

print(test_error_rate_mat)